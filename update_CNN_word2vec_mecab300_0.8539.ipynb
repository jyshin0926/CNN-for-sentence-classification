{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.2-cp36-cp36m-macosx_10_9_x86_64.whl (28.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.8 MB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3\n",
      "  Downloading numpy-1.19.1-cp36-cp36m-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=c8de27c0b7279404b1c94ab151ae49b056a93600ed563cb57b21e38faf9c371c\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, numpy, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.16.0 numpy-1.19.1 scikit-learn-0.23.2 scipy-1.5.2 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from keras) (1.5.2)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-macosx_10_6_intel.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from keras) (1.19.1)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from h5py->keras) (1.15.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-macosx_10_9_x86_64.whl size=44625 sha256=ec3af499ba6184a4a8332f13c26187512c905f759a7b5a0b8d073336cc49e62b\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: h5py, pyyaml, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.4.3 pyyaml-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-macosx_10_11_x86_64.whl (165.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.1 MB 13.4 MB/s eta 0:00:01    |████████████████████████        | 124.0 MB 8.2 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-macosx_10_6_intel.whl (28.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.12.4-cp36-cp36m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.31.0-cp36-cp36m-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from protobuf>=3.9.2->tensorflow) (49.2.1.post20200807)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.20.1-py2.py3-none-any.whl (91 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: wrapt, termcolor, absl-py\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-macosx_10_9_x86_64.whl size=32325 sha256=947b7fb68e12a846f17f3ff574cb91da0138731f37a642a6c7a221feeb684d87\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=f1bd5098e8257c6b5d6695086986a9c1a3026e928f312c6a6e31cbfcdeca906a\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=ce5e93390c8f06397efa37472bc02b1fbe5ae5b1418f1bef4070e478cd4201d1\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "Successfully built wrapt termcolor absl-py\n",
      "Installing collected packages: numpy, scipy, astunparse, protobuf, opt-einsum, google-pasta, keras-preprocessing, grpcio, oauthlib, urllib3, idna, chardet, requests, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, markdown, absl-py, tensorboard-plugin-wit, werkzeug, tensorboard, wrapt, termcolor, tensorflow-estimator, gast, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 chardet-3.0.4 gast-0.3.3 google-auth-1.20.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.31.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.12.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.10 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.0-cp36-cp36m-macosx_10_9_x86_64.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.0 pytz-2020.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Using cached smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: requests in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Collecting boto\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.14.39.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Collecting botocore<1.18.0,>=1.17.39\n",
      "  Downloading botocore-1.17.39-py2.py3-none-any.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.39->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=524e709ed110fdf27116c2c1aafb82171f7f0d85844ac72f53276b11c0be5264\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "  Building wheel for boto3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for boto3: filename=boto3-1.14.39-py2.py3-none-any.whl size=127851 sha256=8c7c1ac25dfdcd556d042404803a8ac3c0525e1a91a4853c191c6732313989c8\n",
      "  Stored in directory: /Users/jaeyoungshin/Library/Caches/pip/wheels/bf/33/3d/275f82e26540754a295503d3310edfe272c5877dad4e3e2d93\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.14.39 botocore-1.17.39 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Using cached tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from konlpy) (1.18.5)\n",
      "Collecting lxml>=4.1.0\n",
      "  Downloading lxml-4.5.2-cp36-cp36m-macosx_10_9_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.0.2-cp36-cp36m-macosx_10_9_x86_64.whl (401 kB)\n",
      "\u001b[K     |████████████████████████████████| 401 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Using cached colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Using cached beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Using cached typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jaeyoungshin/opt/anaconda3/envs/mulcam/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.10)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tweepy, lxml, typing-extensions, JPype1, colorama, beautifulsoup4, konlpy, PySocks\n",
      "Successfully installed JPype1-1.0.2 PySocks-1.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 lxml-4.5.2 tweepy-3.9.0 typing-extensions-3.7.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from konlpy.tag import Mecab\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model (300 dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = models.word2vec.Word2Vec.load('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/mecab_model_files/word2vec_ modeling/NaverMovie_mecab300.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/ratings_train.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "test_data = pd.read_csv('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/ratings_test.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "\n",
    "training_sentences = []\n",
    "testing_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df = pd.read_excel('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/stopwords.xlsx')\n",
    "stopwords_list = stopwords_df['불용어'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for item in train_data.document:\n",
    "    documents.append([word for word in [tokens[0] for tokens in mecab.pos(item)] if word not in stopwords_list])\n",
    "\n",
    "labels = []\n",
    "for item in train_data.label:\n",
    "    labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "for item in test_data.document:\n",
    "    test_documents.append([word for word in [tokens[0] for tokens in mecab.pos(item)] if word not in stopwords_list])\n",
    "\n",
    "test_labels = []\n",
    "for item in test_data.label:\n",
    "    test_labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(labels)\n",
    "testing_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 300\n",
    "max_length = 41\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(test_documents)\n",
    "test_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "text_sequences = tokenizer.texts_to_sequences(documents)\n",
    "padded = pad_sequences(text_sequences,maxlen=max_length,truncating = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save tokenizer as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_mecab300_l2.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save parameters as json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "file_data = OrderedDict()\n",
    "file_data['max_len'] = 41\n",
    "file_data['pad_type'] = \"pre\"\n",
    "file_data['trunc_type'] = \"pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t\"max_len\": 41,\n",
      "\t\"pad_type\": \"pre\",\n",
      "\t\"trunc_type\": \"pre\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(file_data, ensure_ascii=False, indent='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_mecab300_l2.json','w',encoding='utf-8') as make_file:\n",
    "    json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "index = 0 \n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = mecab_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = tf.keras.Input(shape=(max_length,), dtype='int32')\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "for fsz in filter_sizes:\n",
    "    x = tf.keras.layers.Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    convs.append(x)\n",
    "x = tf.keras.layers.Concatenate(axis=-1)(convs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, output)\n",
    "model.layers[1].set_weights([embedding_matrix])\n",
    "model.layers[1].trainable=False\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,patience=1,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 149s 32ms/step - loss: 0.4662 - accuracy: 0.8321 - val_loss: 0.4384 - val_accuracy: 0.8428\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 185s 40ms/step - loss: 0.4325 - accuracy: 0.8526 - val_loss: 0.4501 - val_accuracy: 0.8440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff705f8f070>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[es])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 ]\n",
      " [0.4 ]\n",
      " [0.65]\n",
      " [0.64]\n",
      " [0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"수면제인가 꿀잠잤음\",\"바보같은 영화\",\"조금 기대 이하이긴 했는데 그래도 만족함\", \"완전 재밌었다\", \"개꿀잼\"]\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(model.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: word2vec_mecab300_updated_l2_w2v/0.8440_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('word2vec_mecab300_updated_l2_w2v/0.8440_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load saved model / re-training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = tf.keras.models.load_model('word2vec_mecab300_updated_l2_w2v/0.8440_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "re_es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 484s 103ms/step - loss: 0.4229 - accuracy: 0.8600 - val_loss: 0.4238 - val_accuracy: 0.8572\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 494s 105ms/step - loss: 0.3545 - accuracy: 0.8952 - val_loss: 0.4464 - val_accuracy: 0.8539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff70912dbb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_history = mecab_model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[re_es])\n",
    "re_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**re-training model save (val_accuracy : 0.8539)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word2vec_mecab300_updated_l2_w2v/0.8539_model/assets\n"
     ]
    }
   ],
   "source": [
    "mecab_model.save('word2vec_mecab300_updated_l2_w2v/0.8539_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_remodel = tf.keras.models.load_model('word2vec_mecab300_updated_l2_w2v/0.8539_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99]\n",
      " [0.05]\n",
      " [0.02]\n",
      " [0.76]]\n"
     ]
    }
   ],
   "source": [
    "# 0.7993\n",
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57]\n",
      " [0.63]\n",
      " [0.08]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# 0.8468\n",
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66]\n",
      " [0.51]\n",
      " [0.04]\n",
      " [0.88]]\n"
     ]
    }
   ],
   "source": [
    "# 0.8539\n",
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
