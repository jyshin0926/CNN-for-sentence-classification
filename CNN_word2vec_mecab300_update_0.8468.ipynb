{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from konlpy.tag import Mecab\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model (300 dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = models.word2vec.Word2Vec.load('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/mecab_model_files/word2vec_ modeling/NaverMovie_mecab300.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/ratings_train.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "test_data = pd.read_csv('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/ratings_test.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "\n",
    "training_sentences = []\n",
    "testing_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df = pd.read_excel('/Users/jaeyoungshin/Desktop/NLP/cnn_2조/prep for modeling/stopwords.xlsx')\n",
    "stopwords_list = stopwords_df['불용어'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for item in train_data.document:\n",
    "    documents.append([word for word in [tokens[0] for tokens in mecab.pos(item)] if word not in stopwords_list])\n",
    "\n",
    "labels = []\n",
    "for item in train_data.label:\n",
    "    labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "for item in test_data.document:\n",
    "    test_documents.append([word for word in [tokens[0] for tokens in mecab.pos(item)] if word not in stopwords_list])\n",
    "\n",
    "test_labels = []\n",
    "for item in test_data.label:\n",
    "    test_labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(labels)\n",
    "testing_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 300\n",
    "max_length = 41\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(test_documents)\n",
    "test_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "text_sequences = tokenizer.texts_to_sequences(documents)\n",
    "padded = pad_sequences(text_sequences,maxlen=max_length,truncating = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save tokenizer as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_mecab300_l2.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save parameters as json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "file_data = OrderedDict()\n",
    "file_data['max_len'] = 41\n",
    "file_data['pad_type'] = \"pre\"\n",
    "file_data['trunc_type'] = \"pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t\"max_len\": 41,\n",
      "\t\"pad_type\": \"pre\",\n",
      "\t\"trunc_type\": \"pre\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(file_data, ensure_ascii=False, indent='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_mecab300_l2.json','w',encoding='utf-8') as make_file:\n",
    "    json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "index = 0 \n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = mecab_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = tf.keras.Input(shape=(max_length,), dtype='int32')\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "for fsz in filter_sizes:\n",
    "    x = tf.keras.layers.Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    convs.append(x)\n",
    "x = tf.keras.layers.Concatenate(axis=-1)(convs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,patience=1,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 120s 26ms/step - loss: 0.5821 - accuracy: 0.7503 - val_loss: 0.5277 - val_accuracy: 0.7941\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 121s 26ms/step - loss: 0.5168 - accuracy: 0.8131 - val_loss: 0.5141 - val_accuracy: 0.8150\n",
      "Epoch 3/30\n",
      "4688/4688 [==============================] - 121s 26ms/step - loss: 0.5019 - accuracy: 0.8355 - val_loss: 0.5243 - val_accuracy: 0.8164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f2896a0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[es])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65]\n",
      " [0.52]\n",
      " [0.66]\n",
      " [0.64]\n",
      " [0.49]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"수면제인가 꿀잠잤음\",\"바보같은 영화\",\"조금 기대 이하이긴 했는데 그래도 만족함\", \"완전 재밌었다\", \"개꿀잼\"]\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(model.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: word2vec_mecab300_updated_l2/0.8164_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('word2vec_mecab300_updated_l2/0.8164_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load saved model / re-training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = tf.keras.models.load_model('word2vec_mecab300_updated_l2/0.8164_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "re_es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 349s 75ms/step - loss: 0.5169 - accuracy: 0.8350 - val_loss: 0.4684 - val_accuracy: 0.8474\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 349s 74ms/step - loss: 0.4069 - accuracy: 0.8859 - val_loss: 0.4815 - val_accuracy: 0.8468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f28d05fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_history = mecab_model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[re_es])\n",
    "re_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**re-training model save (val_accuracy : 0.8468)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word2vec_mecab300_updated_l2/0.8468_model/assets\n"
     ]
    }
   ],
   "source": [
    "mecab_model.save('word2vec_mecab300_updated_l2/0.8468_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_remodel = tf.keras.models.load_model('word2vec_mecab300_updated_l2/0.8468_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99]\n",
      " [0.05]\n",
      " [0.02]\n",
      " [0.76]]\n"
     ]
    }
   ],
   "source": [
    "# 0.7993\n",
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57]\n",
      " [0.63]\n",
      " [0.08]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# 0.8468\n",
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
