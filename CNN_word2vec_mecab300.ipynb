{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model (300 dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = models.word2vec.Word2Vec.load('NaverMovie_mecab300.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('ratings_train.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "test_data = pd.read_csv('ratings_test.txt',sep='\\t').dropna().reset_index(drop=True)\n",
    "\n",
    "training_sentences = []\n",
    "testing_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for item in train_data.document:\n",
    "    documents.append(item)\n",
    "\n",
    "labels = []\n",
    "for item in train_data.label:\n",
    "    labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = []\n",
    "for item in test_data.document:\n",
    "    test_documents.append(item)\n",
    "\n",
    "test_labels = []\n",
    "for item in test_data.label:\n",
    "    test_labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(labels)\n",
    "testing_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 300\n",
    "max_length = 41\n",
    "trunc_type='post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "text_sequences = tokenizer.texts_to_sequences(documents)\n",
    "padded = pad_sequences(text_sequences,maxlen=max_length,truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(test_documents)\n",
    "test_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296311"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0   24  937    5 6728 1098]\n",
      "(149995, 41)\n"
     ]
    }
   ],
   "source": [
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "index = 0 \n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = mecab_model.wv.__getitem__(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = tf.keras.Input(shape=(max_length,), dtype='int32')\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "for fsz in filter_sizes:\n",
    "    x = tf.keras.layers.Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    convs.append(x)\n",
    "x = tf.keras.layers.Concatenate(axis=-1)(convs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,patience=1,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 114s 24ms/step - loss: 0.5569 - accuracy: 0.6918 - val_loss: 0.5120 - val_accuracy: 0.7240\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 113s 24ms/step - loss: 0.4278 - accuracy: 0.7840 - val_loss: 0.4815 - val_accuracy: 0.7560\n",
      "Epoch 3/30\n",
      "4688/4688 [==============================] - 112s 24ms/step - loss: 0.3401 - accuracy: 0.8308 - val_loss: 0.5149 - val_accuracy: 0.7562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd40ea9e490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[es])\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49]\n",
      " [0.39]\n",
      " [0.74]\n",
      " [0.91]\n",
      " [0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"수면제인가 꿀잠잤음\",\"바보같은 영화\",\"조금 기대 이하이긴 했는데 그래도 만족함\", \"완전 재밌었다\", \"개꿀잼\"]\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(model.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/jaeyoungshin/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: word2vec_mecab300_model/jy_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('word2vec_mecab300_model/jy_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34massets\u001b[m\u001b[m         saved_model.pb \u001b[34mvariables\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls word2vec_mecab_model/jy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load saved model / re-training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_model = tf.keras.models.load_model('word2vec_mecab300_model/jy_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "re_es = EarlyStopping(monitor='val_accuracy',min_delta=0.002,patience=1,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4688/4688 [==============================] - 365s 78ms/step - loss: 0.3883 - accuracy: 0.8160 - val_loss: 0.4011 - val_accuracy: 0.8054\n",
      "Epoch 2/30\n",
      "4688/4688 [==============================] - 374s 80ms/step - loss: 0.2323 - accuracy: 0.8897 - val_loss: 0.4776 - val_accuracy: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd41bd35280>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_history = mecab_model.fit(padded, training_labels_final, epochs = num_epochs, validation_data = (test_padded, testing_labels_final), callbacks=[re_es])\n",
    "re_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**re-training model save (val_accuracy : 0.7993)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word2vec_mecab300_model/0.7993_model/assets\n"
     ]
    }
   ],
   "source": [
    "mecab_model.save('word2vec_mecab300_model/0.7993_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_remodel = tf.keras.models.load_model('word2vec_mecab300_model/0.7993_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49]\n",
      " [0.16]\n",
      " [0.01]\n",
      " [0.86]\n",
      " [0.99]\n",
      " [0.96]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"수면제인가 꿀잠잤음 개꿀\",\"바보같은 영화\",\"바보같은 영화네\",\"조금 기대 이하이긴 했는데 그래도 만족함\", \"완전 재밌었다\", \"개꿀잼\"]\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99]\n",
      " [0.05]\n",
      " [0.02]\n",
      " [0.76]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"완전 추천함\",\"아 겁나 지루했음 개비추..\",\"에라이 때려쳐 이게 영화라고?\",'내 인생 영화']\n",
    "sequence_exp = tokenizer.texts_to_sequences(sentence)\n",
    "padded_exp = pad_sequences(sequence_exp, maxlen = max_length, truncating= 'post')\n",
    "print(mecab_remodel.predict(padded_exp).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
